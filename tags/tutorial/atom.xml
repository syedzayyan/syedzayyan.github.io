<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Syed Zayyan Masud - tutorial?</title>
	<subtitle>A personal portfolio&#x2F;blog website of Syed Zayyan Masud</subtitle>
	<link href="https://syedzayyan.com/tags/tutorial/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://syedzayyan.com"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2026-02-11T14:00:01+00:00</updated>
	<id>https://syedzayyan.com/tags/tutorial/atom.xml</id>
	<entry xml:lang="en">
		<title>Temporal Point Processes and Screaming Neighbours</title>
		<published>2026-02-11T14:00:01+00:00</published>
		<updated>2026-02-11T14:00:01+00:00</updated>
		<link rel="alternate" href="https://syedzayyan.com/posts/tpp/" type="text/html"/>
		<id>https://syedzayyan.com/posts/tpp/</id>
		<content type="html">&lt;p&gt;I have been investigating into time-series models and landed onto Temporal Point Process one fine day. This world mostly concerts itself with finance and I am not a finance person or a statistics person. This is a tutorial or more appropiately my thoughts on how all of this works. Before I move along here is the problem we will be working with today.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;&#x2F;h3&gt;
&lt;p&gt;The issue at hand is my neighbour screams loudly everyday. Once only in a day but almost all days of the year. To simulate my neighbour doing professional neighbouring I am using a &lt;strong&gt;Hawkes Process&lt;&#x2F;strong&gt;. I will be using &lt;strong&gt;Recurrent Marked Temporal Processes&lt;&#x2F;strong&gt; to predict when my neighbour is going to scream again next, so that I am prepare.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
import torch.nn.functional as F
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)
print(f&amp;quot;Using device: {device}&amp;quot;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;simulate-scream-data&quot;&gt;SIMULATE SCREAM DATA&lt;&#x2F;h3&gt;
&lt;p&gt;Before I explain what Hawkes Processes are, some background information is in order.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Stochastic processes&lt;&#x2F;strong&gt; are defined as (often infinite) collections of random variables, indexed by some time or space. In our case it is time, formalizing a model for the “discrete-time” data above, we could write {Xt}∞t=0,t∈Z+. A random variate, or a realization of the process is the entire trajectory determined by values taken by all Xt (often part of which we observe). In other words the scream, they are random, hence random variables. ⭐⭐⭐ &lt;em&gt;statistics&lt;&#x2F;em&gt;. Here Xt make up the collection while Z+ is the index set. The specific dependence (or rather, independence) structure, and other parametric assumptions of relationships among {Xt} determine the stochastic process.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;strong&gt;Counting processes&lt;&#x2F;strong&gt; is a stochastic process { N ( t ) , t ≥ 0 } {\displaystyle {N(t),t\geq 0}} with values that are non-negative, integer, and non-decreasing. In other words we count the number of screams in a day.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;strong&gt;Poisson Process&lt;&#x2F;strong&gt; is a counting process with quirks. Quirks being:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;We define a Poisson process with a function λ(t)&amp;gt;0,∀t&lt;&#x2F;li&gt;
&lt;li&gt;Say we have two intervals, A,B⊂R The number of occurrences in these intervals will be Poisson distributed with N(A)∼∫Aλ(t)dt, and N(B)∼∫Bλ(t)dt&lt;&#x2F;li&gt;
&lt;li&gt;Most importantly, N(A),N(B) are independent variables for all A∩B=∅&lt;&#x2F;li&gt;
&lt;li&gt;Higher intensity functions λ(t)as expected, are associated with higher probabilities of event occurrences.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Imagine the neighbour screams because they are arguing. Arguments rarely end in one shout. One scream makes another scream more likely shortly after. But if a few calm days pass, the probability resets to baseline.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Self-Exciting Processes&lt;&#x2F;strong&gt; Now we allow events to influence future events. One scream increases the probability of another scream shortly after.But that influence decays over time. That’s called self-excitation. If neighbour screams because they are arguing:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;First scream → increases probability of second scream.&lt;&#x2F;li&gt;
&lt;li&gt;After a few calm days → back to normal baseline.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Memory exists, but it fades.&lt;&#x2F;p&gt;
&lt;h5 id=&quot;finally-the-hawkes-processes&quot;&gt;Finally the, Hawkes Processes&lt;&#x2F;h5&gt;
&lt;p&gt;The (univariate) Hawkes process is defined by the conditional intensity function&lt;&#x2F;p&gt;
&lt;p&gt;λ∗(t)=μ+∑ti&amp;lt;tφ(t−ti).&lt;&#x2F;p&gt;
&lt;p&gt;At any moment t, the conditional intensity function is at least μ&amp;gt;0, the background intensity. However, it also depends linearly on effects of events that have occurred before time t. Namely, this dependence is through a triggering kernel function φ(.), a function of the delay t−ti between the current time and the timestamp of the previous event. Note that φ is nonnegative (φ(x)≥0,∀x≥0 and causal φ(x)=0,∀x&amp;lt;0. It is usually a monotonically decreasing function (such as exponential decay, or power-law decay).&lt;&#x2F;p&gt;
&lt;p&gt;Thinking the other way around, the function can be interpreted as follows. Everytime neighbour screams, they&#x27;ll scream more in the next days. &lt;em&gt;Self Exciting&lt;&#x2F;em&gt; called bursts. Probably, if they are sad, they&#x27;ll remain sad for a while?&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;def simulate_hawkes(T=365.0, mu=0.4, alpha=0.8, beta=1.5, seed=42):
    np.random.seed(seed)
    t = 0.0
    events = []

    while t &amp;lt; T:
        if len(events) == 0:
            lambda_bar = mu
        else:
            lambda_bar = mu + alpha * np.sum(np.exp(-beta * (t - np.array(events))))

        u = np.random.rand()
        w = -np.log(u) &#x2F; lambda_bar
        t += w

        if t &amp;gt;= T:
            break

        lambda_t = mu + alpha * np.sum(np.exp(-beta * (t - np.array(events))))
        d = np.random.rand()

        if d * lambda_bar &amp;lt;= lambda_t:
            events.append(t)

    return np.array(events)

# Generate full data
true_times = simulate_hawkes()
gap_start, gap_end = 100.0, 130.0  # 30-day logger gap
latent_times = true_times[(true_times &amp;gt;= gap_start) &amp;amp; (true_times &amp;lt; gap_end)]
latent_count = len(latent_times)
obs_times = np.concatenate([
    true_times[true_times &amp;lt; gap_start],
    true_times[true_times &amp;gt;= gap_end]
])

print(f&amp;quot;True screams: {len(true_times)}, Observed: {len(obs_times)}, Latent in gap: {latent_count}&amp;quot;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Yes my neighbour seems to scream a lot&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;plt.figure(figsize=(12, 3))

# Event (raster) plot
plt.plot(true_times, np.zeros_like(true_times), &amp;#39;|&amp;#39;, markersize=12)

plt.xlabel(&amp;quot;Time (days)&amp;quot;)
plt.yticks([])
plt.title(&amp;quot;Simulated Neighbour Screams (Hawkes Process)&amp;quot;)

plt.tight_layout()
plt.show()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;parametric-hawkes-baseline&quot;&gt;PARAMETRIC HAWKES BASELINE&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;class Hawkes(nn.Module):
    def __init__(self):
        super().__init__()
        self.mu = nn.Parameter(torch.tensor(0.3))
        self.alpha = nn.Parameter(torch.tensor(0.5))
        self.beta = nn.Parameter(torch.tensor(1.0))

    def intensity(self, t, history):
        past = history[history &amp;lt; t]
        if len(past) == 0:
            return self.mu
        return self.mu + self.alpha * torch.sum(
            torch.exp(-self.beta * (t - past))
        )

    def integral(self, a, b, history):
        mu_part = self.mu * (b - a)
        past = history[history &amp;lt; b]

        if len(past) == 0:
            return mu_part

        decay_a = torch.sum(torch.exp(-self.beta * (a - past)))
        decay_b = torch.sum(torch.exp(-self.beta * (b - past)))
        exc = (self.alpha &#x2F; self.beta) * (decay_a - decay_b)

        return mu_part + exc

    def log_likelihood(self, times):
        ll = 0.0
        prev_t = torch.tensor(0.0, device=times.device)

        for i, t in enumerate(times):
            lamb = self.intensity(t, times[:i])
            integ = self.integral(prev_t, t, times[:i])
            ll += torch.log(lamb + 1e-8) - integ
            prev_t = t

        # subtract integral to T
        T = times[-1]
        ll -= self.integral(prev_t, T, times)

        return ll


def train_hawkes(model, times, epochs=400):
    optimizer = optim.Adam(model.parameters(), lr=0.05)
    times_t = torch.tensor(times, dtype=torch.float32, device=device)

    for e in range(epochs):
        optimizer.zero_grad()
        ll = model.log_likelihood(times_t)
        loss = -ll
        loss.backward()
        optimizer.step()

        if e % 100 == 0:
            print(&amp;quot;Hawkes NLL:&amp;quot;, -ll.item())

    br = model.alpha.item() &#x2F; model.beta.item()
    print(&amp;quot;Branching ratio α&#x2F;β:&amp;quot;, br)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;rmtpp-model-recurrent-marked-tpp-simplified-univariate&quot;&gt;RMTPP MODEL (Recurrent Marked TPP, simplified univariate)&lt;&#x2F;h3&gt;
&lt;!----&gt;
&lt;p&gt;So far, Hawkes assumed memory decays exponentially. But what if your neighbour’s emotional state is not exponential? Instead of hard-coding the memory kernel, we can let a neural network learn it.&lt;&#x2F;p&gt;
&lt;p&gt;That is RMTPP.&lt;&#x2F;p&gt;
&lt;p&gt;We observe inter-event times:
$$
\tau_j = t_j - t_{j-1}
$$
These are fed into an RNN (typically an LSTM), producing hidden states:
$$
h_j = \mathrm{RNN}(h_{j-1}, \tau_j)
$$&lt;&#x2F;p&gt;
&lt;p&gt;The hidden state $h_j$ is a learned summary of the entire past. This replaces the explicit Hawkes kernel. The &lt;a href=&quot;https:&#x2F;&#x2F;www.kdd.org&#x2F;kdd2016&#x2F;papers&#x2F;files&#x2F;rpp1081-duA.pdf&quot;&gt;original paper&lt;&#x2F;a&gt; is a way better resource. Please head over there if you want more insights.&lt;&#x2F;p&gt;
&lt;!----&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Hawkes&lt;&#x2F;th&gt;&lt;th&gt;RMTPP&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Exponential memory kernel&lt;&#x2F;td&gt;&lt;td&gt;Learned memory&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Interpretable parameters&lt;&#x2F;td&gt;&lt;td&gt;Black box&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Parametric&lt;&#x2F;td&gt;&lt;td&gt;Neural&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Stable if α&#x2F;β &amp;lt; 1&lt;&#x2F;td&gt;&lt;td&gt;Stability learned implicitly&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;class RMTPP(nn.Module):
    def __init__(self, hidden_dim=32):
        super().__init__()
        self.lstm = nn.LSTM(1, hidden_dim, batch_first=True)
        self.v = nn.Linear(hidden_dim, 1)
        self.w = nn.Parameter(torch.tensor(0.1))
        self.b = nn.Parameter(torch.tensor(0.0))

    def forward_history(self, deltas):
        deltas = deltas.unsqueeze(-1)
        out, _ = self.lstm(deltas)
        return out

    def intensity(self, h, tau):
        return torch.exp(self.v(h) + self.w * tau + self.b)

    def log_density(self, h, tau):
        a = self.v(h) + self.b
        w = self.w

        lamb = torch.exp(a + w * tau)
        integral = (torch.exp(a) &#x2F; w) * (torch.exp(w * tau) - 1)

        return torch.log(lamb + 1e-8) - integral

    def log_likelihood(self, deltas):
        h_all = self.forward_history(deltas.unsqueeze(0))[0]

        ll = 0.0
        for j in range(1, len(deltas)):
            h_prev = h_all[j - 1]
            tau = deltas[j]
            ll += self.log_density(h_prev, tau)

        return ll
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;training-hawkes-rmtpp&quot;&gt;TRAINING (Hawkes + RMTPP)&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;def train_rmtpp(model, times, epochs=600):
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    deltas = np.diff(np.concatenate([[0.0], times]))
    deltas_t = torch.tensor(deltas, dtype=torch.float32, device=device)

    for e in range(epochs):
        optimizer.zero_grad()
        ll = model.log_likelihood(deltas_t)
        loss = -ll
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        if e % 150 == 0:
            print(&amp;quot;RMTPP NLL:&amp;quot;, -ll.item())

def sample_rmtpp_next(model, times):
    deltas = np.diff(np.concatenate([[0.0], times]))
    deltas_t = torch.tensor(deltas, dtype=torch.float32, device=device)

    h_all = model.forward_history(deltas_t.unsqueeze(0))[0]
    h_last = h_all[-1]

    a = model.v(h_last) + model.b
    w = model.w

    u = torch.rand(1, device=device)

    tau = (1.0 &#x2F; w) * torch.log(
        1 - (w &#x2F; torch.exp(a)) * torch.log(1 - u)
    )

    return tau.item()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;hawkes = Hawkes().to(device)
train_hawkes(hawkes, true_times)

rmtpp = RMTPP().to(device)
train_rmtpp(rmtpp, true_times)

next_tau = sample_rmtpp_next(rmtpp, true_times)
print(&amp;quot;Predicted next scream in ~&amp;quot;, next_tau, &amp;quot;days&amp;quot;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;plot-results&quot;&gt;PLOT RESULTS&lt;&#x2F;h3&gt;
&lt;pre data-lang=&quot;python {.marimo}&quot; class=&quot;language-python {.marimo} z-code&quot;&gt;&lt;code class=&quot;language-python {.marimo}&quot; data-lang=&quot;python {.marimo}&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;t_eval = np.linspace(true_times[-1], true_times[-1] + 10, 200)
history_t = torch.tensor(true_times, dtype=torch.float32)

hawkes_int = [
    hawkes.intensity(torch.tensor(t), history_t).item()
    for t in t_eval
]

plt.figure(figsize=(10,5))
plt.plot(t_eval, hawkes_int)
plt.title(&amp;quot;Hawkes Predicted Intensity&amp;quot;)
plt.xlabel(&amp;quot;Time&amp;quot;)
plt.ylabel(&amp;quot;Intensity&amp;quot;)
plt.show()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;References:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;shchur.github.io&#x2F;blog&#x2F;2020&#x2F;tpp1-conditional-intensity&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;hawkeslib.readthedocs.io&#x2F;en&#x2F;latest&#x2F;tutorial.html#footnote-reference-1&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Counting_process&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
	</entry>
</feed>
